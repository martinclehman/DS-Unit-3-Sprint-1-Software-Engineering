{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrzN2H1vNe1H"
   },
   "source": [
    "_Lambda School Data Science ‚Äî Linear Models_\n",
    "\n",
    "# Intro to Predictive Modeling\n",
    "\n",
    "### Objectives\n",
    "- recognize examples of supervised learning with tabular data\n",
    "- distinguish between regression problems and classification problems\n",
    "- explain why overfitting is a problem and model validation is important\n",
    "- do train/test split\n",
    "- begin with baselines for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTvY3lTKGlZm"
   },
   "source": [
    "I like Brandon Rohrer‚Äôs blog post, [‚ÄúWhat questions can machine learning answer?‚Äù](https://brohrer.github.io/five_questions_data_science_answers.html)\n",
    "\n",
    "We‚Äôll focus on two of these questions in Unit 2. These are both types of ‚Äúsupervised learning.‚Äù\n",
    "\n",
    "- ‚ÄúIs this A or B?‚Äù (Classification)\n",
    "- ‚ÄúHow Much / How Many?‚Äù (Regression)\n",
    "\n",
    "**This unit, you‚Äôll do four supervised learning projects** with ‚Äútabular data‚Äù (data in tables, like spreadsheets).\n",
    "\n",
    "- Predict New York City apartment rents <-- **Today, we'll start this project!**\n",
    "- Predict which water pumps in Tanzania need repairs\n",
    "- Predict the prices suppliers will quote Caterpillar for industrial parts\n",
    "- Choose your own labeled, tabular dataset, train a predictive model, and publish a blog post or web app with visualizations to explain your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HthEENh8zCa"
   },
   "source": [
    "# Predict NYC apartment rent üè†üí∏\n",
    "\n",
    "You'll use a real-world data with rent prices for a subset of apartments in New York City!\n",
    "\n",
    "Run this code cell to load the dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ge86BI8iN6Wy"
   },
   "source": [
    "### Install [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling), version >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "k6y_YIiJMJG_",
    "outputId": "c8f3b5e4-1c3c-47a6-837b-5edd9cf85591"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mmGiyke6MNkQ",
    "outputId": "92e0269c-5265-4a1f-93a5-1f13f330f95c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas_profiling\n",
    "pandas_profiling.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yrr19LoD8Y0E"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LOCAL = '../data/nyc/nyc-rent-2016.csv'\n",
    "WEB = 'https://raw.githubusercontent.com/martinclehman/DS-Unit-2-Linear-Models/master/data/nyc/nyc-rent-2016.csv'\n",
    "\n",
    "df = pd.read_csv(WEB)\n",
    "#df = pd.read_csv(LOCAL)  #for local jupyter instance\n",
    "assert df.shape == (48300, 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pdg1QqZP-DWL"
   },
   "source": [
    "## Define the problem\n",
    "- Is this **supervised** learning?\n",
    "- Is this **tabular** data?\n",
    "- Is this **regression** or **classification**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "fwDBjkVb9AGa",
    "outputId": "02bdf7f3-db85-4d80-e696-56db461293d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48300 entries, 0 to 48299\n",
      "Data columns (total 34 columns):\n",
      "bathrooms               48300 non-null float64\n",
      "bedrooms                48300 non-null int64\n",
      "created                 48300 non-null object\n",
      "description             46879 non-null object\n",
      "display_address         48168 non-null object\n",
      "latitude                48300 non-null float64\n",
      "longitude               48300 non-null float64\n",
      "price                   48300 non-null int64\n",
      "street_address          48290 non-null object\n",
      "interest_level          48300 non-null object\n",
      "elevator                48300 non-null int64\n",
      "cats_allowed            48300 non-null int64\n",
      "hardwood_floors         48300 non-null int64\n",
      "dogs_allowed            48300 non-null int64\n",
      "doorman                 48300 non-null int64\n",
      "dishwasher              48300 non-null int64\n",
      "no_fee                  48300 non-null int64\n",
      "laundry_in_building     48300 non-null int64\n",
      "fitness_center          48300 non-null int64\n",
      "pre-war                 48300 non-null int64\n",
      "laundry_in_unit         48300 non-null int64\n",
      "roof_deck               48300 non-null int64\n",
      "outdoor_space           48300 non-null int64\n",
      "dining_room             48300 non-null int64\n",
      "high_speed_internet     48300 non-null int64\n",
      "balcony                 48300 non-null int64\n",
      "swimming_pool           48300 non-null int64\n",
      "new_construction        48300 non-null int64\n",
      "exclusive               48300 non-null int64\n",
      "terrace                 48300 non-null int64\n",
      "loft                    48300 non-null int64\n",
      "garden_patio            48300 non-null int64\n",
      "common_outdoor_space    48300 non-null int64\n",
      "wheelchair_access       48300 non-null int64\n",
      "dtypes: float64(3), int64(26), object(5)\n",
      "memory usage: 12.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "awBKFMz0vubE",
    "outputId": "9ea1590e-6e1d-4948-ae4d-e97b5632e16b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>elevator</th>\n",
       "      <th>cats_allowed</th>\n",
       "      <th>hardwood_floors</th>\n",
       "      <th>dogs_allowed</th>\n",
       "      <th>doorman</th>\n",
       "      <th>...</th>\n",
       "      <th>high_speed_internet</th>\n",
       "      <th>balcony</th>\n",
       "      <th>swimming_pool</th>\n",
       "      <th>new_construction</th>\n",
       "      <th>exclusive</th>\n",
       "      <th>terrace</th>\n",
       "      <th>loft</th>\n",
       "      <th>garden_patio</th>\n",
       "      <th>common_outdoor_space</th>\n",
       "      <th>wheelchair_access</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "      <td>48300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.178313</td>\n",
       "      <td>1.508799</td>\n",
       "      <td>40.750782</td>\n",
       "      <td>-73.972365</td>\n",
       "      <td>3438.297950</td>\n",
       "      <td>0.520393</td>\n",
       "      <td>0.477702</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.446832</td>\n",
       "      <td>0.417805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085839</td>\n",
       "      <td>0.058509</td>\n",
       "      <td>0.053002</td>\n",
       "      <td>0.050476</td>\n",
       "      <td>0.043623</td>\n",
       "      <td>0.043851</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.037992</td>\n",
       "      <td>0.026356</td>\n",
       "      <td>0.026066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.426120</td>\n",
       "      <td>1.092232</td>\n",
       "      <td>0.039560</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>1401.422247</td>\n",
       "      <td>0.499589</td>\n",
       "      <td>0.499508</td>\n",
       "      <td>0.499358</td>\n",
       "      <td>0.497170</td>\n",
       "      <td>0.493203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280128</td>\n",
       "      <td>0.234706</td>\n",
       "      <td>0.224040</td>\n",
       "      <td>0.218928</td>\n",
       "      <td>0.204257</td>\n",
       "      <td>0.204765</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.191178</td>\n",
       "      <td>0.160194</td>\n",
       "      <td>0.159334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.571200</td>\n",
       "      <td>-74.094000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.728100</td>\n",
       "      <td>-73.991700</td>\n",
       "      <td>2495.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.751600</td>\n",
       "      <td>-73.977900</td>\n",
       "      <td>3100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.774000</td>\n",
       "      <td>-73.954700</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>40.915400</td>\n",
       "      <td>-73.700100</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          bathrooms      bedrooms      latitude     longitude         price  \\\n",
       "count  48300.000000  48300.000000  48300.000000  48300.000000  48300.000000   \n",
       "mean       1.178313      1.508799     40.750782    -73.972365   3438.297950   \n",
       "std        0.426120      1.092232      0.039560      0.029563   1401.422247   \n",
       "min        0.000000      0.000000     40.571200    -74.094000   1025.000000   \n",
       "25%        1.000000      1.000000     40.728100    -73.991700   2495.000000   \n",
       "50%        1.000000      1.000000     40.751600    -73.977900   3100.000000   \n",
       "75%        1.000000      2.000000     40.774000    -73.954700   4000.000000   \n",
       "max       10.000000      8.000000     40.915400    -73.700100   9999.000000   \n",
       "\n",
       "           elevator  cats_allowed  hardwood_floors  dogs_allowed  \\\n",
       "count  48300.000000  48300.000000     48300.000000  48300.000000   \n",
       "mean       0.520393      0.477702         0.474576      0.446832   \n",
       "std        0.499589      0.499508         0.499358      0.497170   \n",
       "min        0.000000      0.000000         0.000000      0.000000   \n",
       "25%        0.000000      0.000000         0.000000      0.000000   \n",
       "50%        1.000000      0.000000         0.000000      0.000000   \n",
       "75%        1.000000      1.000000         1.000000      1.000000   \n",
       "max        1.000000      1.000000         1.000000      1.000000   \n",
       "\n",
       "            doorman        ...          high_speed_internet       balcony  \\\n",
       "count  48300.000000        ...                 48300.000000  48300.000000   \n",
       "mean       0.417805        ...                     0.085839      0.058509   \n",
       "std        0.493203        ...                     0.280128      0.234706   \n",
       "min        0.000000        ...                     0.000000      0.000000   \n",
       "25%        0.000000        ...                     0.000000      0.000000   \n",
       "50%        0.000000        ...                     0.000000      0.000000   \n",
       "75%        1.000000        ...                     0.000000      0.000000   \n",
       "max        1.000000        ...                     1.000000      1.000000   \n",
       "\n",
       "       swimming_pool  new_construction     exclusive       terrace  \\\n",
       "count   48300.000000      48300.000000  48300.000000  48300.000000   \n",
       "mean        0.053002          0.050476      0.043623      0.043851   \n",
       "std         0.224040          0.218928      0.204257      0.204765   \n",
       "min         0.000000          0.000000      0.000000      0.000000   \n",
       "25%         0.000000          0.000000      0.000000      0.000000   \n",
       "50%         0.000000          0.000000      0.000000      0.000000   \n",
       "75%         0.000000          0.000000      0.000000      0.000000   \n",
       "max         1.000000          1.000000      1.000000      1.000000   \n",
       "\n",
       "               loft  garden_patio  common_outdoor_space  wheelchair_access  \n",
       "count  48300.000000  48300.000000          48300.000000       48300.000000  \n",
       "mean       0.041263      0.037992              0.026356           0.026066  \n",
       "std        0.198900      0.191178              0.160194           0.159334  \n",
       "min        0.000000      0.000000              0.000000           0.000000  \n",
       "25%        0.000000      0.000000              0.000000           0.000000  \n",
       "50%        0.000000      0.000000              0.000000           0.000000  \n",
       "75%        0.000000      0.000000              0.000000           0.000000  \n",
       "max        1.000000      1.000000              1.000000           1.000000  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "colab_type": "code",
    "id": "1uoHl4mlwLDk",
    "outputId": "758cce9d-c3bd-41d4-963e-97e57571e4c7"
   },
   "outputs": [],
   "source": [
    "#df.profile_report()\n",
    "\n",
    "#OR IF YOU WANT TO SAVE PROFILE REPORT TO HTML FILE\n",
    "#profile = df.profile_report()\n",
    "#profile.to_file(output_file=\"output.html\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "colab_type": "code",
    "id": "uIr8drvav_bZ",
    "outputId": "f21a0f8b-13f7-4592-9c75-1101a14fbac1"
   },
   "outputs": [],
   "source": [
    "#df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3WH0KKHwL_5"
   },
   "source": [
    "## Explain why overfitting is a problem and model validation is important\n",
    "\n",
    "#### Jason Brownlee, [Overfitting and Underfitting With Machine Learning Algorithms](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)\n",
    "\n",
    "> The goal of a good machine learning model is to **generalize** well from the training data to any data from the problem domain. This allows us to make predictions in the future on data the model has never seen.\n",
    "\n",
    "> The cause of poor performance in machine learning is either overfitting or underfitting the data.\n",
    "\n",
    "> **Overfitting** refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. \n",
    "\n",
    "> **Underfitting** refers to a model that can neither model the training data nor generalize to new data.\n",
    "\n",
    "> Ideally, you want to select a model at the sweet spot between underfitting and overfitting.\n",
    "\n",
    "\n",
    "#### Rob Hyndman & George Athanasopoulos, [_Forecasting: Principles and Practice_, Chapter 3.4](https://otexts.com/fpp2/accuracy.html), Evaluating forecast accuracy:\n",
    "\n",
    "> The following points should be noted.\n",
    "\n",
    "> - A model which fits the training data well will not necessarily forecast well.\n",
    "> - A perfect fit can always be obtained by using a model with enough parameters.\n",
    "> - Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data.\n",
    "\n",
    "> **The accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when fitting the model.**\n",
    "\n",
    "> When choosing models, it is common practice to separate the available data into two portions, training and test data, where the training data is used to estimate any parameters of a forecasting method and the test data is used to evaluate its accuracy. Because the test data is not used in determining the forecasts, it should provide a reliable indication of how well the model is likely to forecast on new data.\n",
    "\n",
    "![](https://otexts.com/fpp2/fpp_files/figure-html/traintest-1.png)\n",
    "\n",
    "> The size of the test set is typically about 20% of the total sample, although this value depends on how long the sample is and how far ahead you want to forecast. The test set should ideally be at least as large as the maximum forecast horizon required.\n",
    "\n",
    "> Some references describe the test set as the ‚Äúhold-out set‚Äù because these data are ‚Äúheld out‚Äù of the data used for fitting. Other references call the training set the ‚Äúin-sample data‚Äù and the test set the ‚Äúout-of-sample data‚Äù. We prefer to use ‚Äútraining data‚Äù and ‚Äútest data‚Äù in this book.\n",
    "\n",
    "\n",
    "#### Rachel Thomas, [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/)\n",
    "\n",
    "> An all-too-common scenario: a seemingly impressive machine learning model is a complete failure when implemented in production. The fallout includes leaders who are now skeptical of machine learning and reluctant to try it again. How can this happen?\n",
    "\n",
    "> One of the most likely culprits for this disconnect between results in development vs results in production is a poorly chosen validation set (or even worse, no validation set at all). \n",
    "\n",
    "\n",
    "\n",
    "#### James, Witten, Hastie, Tibshirani, [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 2.2, Assessing Model Accuracy\n",
    "\n",
    "> In general, we do not really care how well the method works training on the training data. Rather, _we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data._ Why is this what we care about? \n",
    "\n",
    "> Suppose that we are interested test data in developing an algorithm to predict a stock‚Äôs price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don‚Äôt really care how well our method predicts last week‚Äôs stock price. We instead care about how well it will predict tomorrow‚Äôs price or next month‚Äôs price. \n",
    "\n",
    "> On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for _future patients_ based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.\n",
    "\n",
    "#### Owen Zhang, [Winning Data Science Competitions](https://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions/8)\n",
    "\n",
    "> Good validation is _more important_ than good models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nidct-New21O"
   },
   "source": [
    "## Do train/test split\n",
    "\n",
    "We have two options for where we choose to split:\n",
    "- Time\n",
    "- Random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pQeWS2nI4nY"
   },
   "source": [
    "This choice depends on your goals. Rachel Thomas explains why you may want to split on time:\n",
    "\n",
    "#### Rachel Thomas, [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/)\n",
    "\n",
    "> If your data is a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates your are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYBgQkz8I5EQ"
   },
   "source": [
    "For this project, we'll split based on time. \n",
    "\n",
    "- Use data from April & May 2016 to train.\n",
    "- Use data from June 2016 to test.\n",
    "\n",
    "(But in some future projects this unit, we'll do random splits, and explain why.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "R6taX496EtRm",
    "outputId": "871d891d-2dc8-4025-b8b8-ab1a08ea4ea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2016-06-24 07:54:24\n",
       "1       2016-06-12 12:19:27\n",
       "2       2016-04-17 03:26:41\n",
       "3       2016-04-18 02:22:02\n",
       "4       2016-04-28 01:32:41\n",
       "5       2016-04-19 04:24:47\n",
       "6       2016-04-27 03:19:56\n",
       "7       2016-04-13 06:01:42\n",
       "8       2016-04-20 02:36:35\n",
       "9       2016-04-02 02:58:15\n",
       "10      2016-04-14 01:10:30\n",
       "11      2016-06-03 03:21:22\n",
       "12      2016-04-19 05:37:25\n",
       "13      2016-04-09 01:22:11\n",
       "14      2016-06-01 03:11:01\n",
       "15      2016-04-18 02:36:00\n",
       "16      2016-04-22 05:57:16\n",
       "17      2016-04-19 03:36:28\n",
       "18      2016-04-20 04:00:32\n",
       "19      2016-04-09 03:49:42\n",
       "20      2016-04-12 02:39:45\n",
       "21      2016-04-07 02:11:45\n",
       "22      2016-04-13 05:17:43\n",
       "23      2016-04-17 02:26:52\n",
       "24      2016-06-07 04:39:56\n",
       "25      2016-04-27 06:43:40\n",
       "26      2016-04-25 02:29:50\n",
       "27      2016-04-22 05:39:10\n",
       "28      2016-04-21 05:27:10\n",
       "29      2016-04-26 02:59:37\n",
       "                ...        \n",
       "48270   2016-06-02 13:24:18\n",
       "48271   2016-04-25 01:13:37\n",
       "48272   2016-04-26 04:43:14\n",
       "48273   2016-04-08 05:51:57\n",
       "48274   2016-04-29 15:03:29\n",
       "48275   2016-04-24 03:15:18\n",
       "48276   2016-04-25 01:20:23\n",
       "48277   2016-04-03 03:51:21\n",
       "48278   2016-04-19 02:58:55\n",
       "48279   2016-04-14 05:39:33\n",
       "48280   2016-06-06 01:22:44\n",
       "48281   2016-04-26 02:18:16\n",
       "48282   2016-04-16 16:39:50\n",
       "48283   2016-04-05 04:35:32\n",
       "48284   2016-04-22 03:11:44\n",
       "48285   2016-04-29 05:20:49\n",
       "48286   2016-04-03 05:23:23\n",
       "48287   2016-04-15 05:36:39\n",
       "48288   2016-04-14 03:32:24\n",
       "48289   2016-04-27 12:52:12\n",
       "48290   2016-04-07 02:29:00\n",
       "48291   2016-04-02 03:17:03\n",
       "48292   2016-04-14 03:39:42\n",
       "48293   2016-04-11 03:29:05\n",
       "48294   2016-04-22 15:44:11\n",
       "48295   2016-06-02 05:41:05\n",
       "48296   2016-04-04 18:22:34\n",
       "48297   2016-04-16 02:13:40\n",
       "48298   2016-04-08 02:13:33\n",
       "48299   2016-04-12 02:48:07\n",
       "Name: created, Length: 48300, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['created'] = pd.to_datetime(df['created'], infer_datetime_format=True)\n",
    "df['created']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'datetime64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dc239ff0355e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'datetime64'"
     ]
    }
   ],
   "source": [
    "df['created'].datetime64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "w7xL3kiw5cNT",
    "outputId": "568fc13c-e3c5-48ae-ccb1-71c5100a7852"
   },
   "outputs": [],
   "source": [
    "df['created'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JU-qSXmt5e9R"
   },
   "outputs": [],
   "source": [
    "#Since dt is 'datetime64' we can extract the month and create a column\n",
    "df['month'] = df['created'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "owolkuwZ5iZx",
    "outputId": "d117d189-796d-4173-be01-a126d1ad718f"
   },
   "outputs": [],
   "source": [
    "df['month'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUXby-3L5kxz"
   },
   "outputs": [],
   "source": [
    "#Now we split the dataframe \n",
    "train = df[df['month'] < 6]\n",
    "test = df[df['month'] == 6]\n",
    "assert train.shape[0] + test.shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Gl68a8UC66TR",
    "outputId": "2a8ae967-9231-4ddb-f644-62e31be9dd2e"
   },
   "outputs": [],
   "source": [
    "train['created'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "lOUuFNmyqXZb",
    "outputId": "008e1032-45ba-4b92-fd16-b022669d81fc"
   },
   "outputs": [],
   "source": [
    "test['created'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anavL_0PwjbR"
   },
   "source": [
    "## Begin with baselines for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7GNLhwjwWHE"
   },
   "source": [
    "### Why begin with baselines?\n",
    "\n",
    "[My mentor](https://www.linkedin.com/in/jason-sanchez-62093847/) [taught me](https://youtu.be/0GrciaGYzV0?t=40s):\n",
    "\n",
    ">***Your first goal should always, always, always be getting a generalized prediction as fast as possible.*** You shouldn't spend a lot of time trying to tune your model, trying to add features, trying to engineer features, until you've actually gotten one prediction, at least. \n",
    "\n",
    "> The reason why that's a really good thing is because then ***you'll set a benchmark*** for yourself, and you'll be able to directly see how much effort you put in translates to a better prediction. \n",
    "\n",
    "> What you'll find by working on many models: some effort you put in, actually has very little effect on how well your final model does at predicting new observations. Whereas some very easy changes actually have a lot of effect. And so you get better at allocating your time more effectively.\n",
    "\n",
    "My mentor's advice is echoed and elaborated in several sources:\n",
    "\n",
    "[Always start with a stupid model, no exceptions](https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa)\n",
    "\n",
    "> Why start with a baseline? A baseline will take you less than 1/10th of the time, and could provide up to 90% of the results. A baseline puts a more complex model into context. Baselines are easy to deploy.\n",
    "\n",
    "[Measure Once, Cut Twice: Moving Towards Iteration in Data Science](https://blog.datarobot.com/measure-once-cut-twice-moving-towards-iteration-in-data-science)\n",
    "\n",
    "> The iterative approach in data science starts with emphasizing the importance of getting to a first model quickly, rather than starting with the variables and features. Once the first model is built, the work then steadily focuses on continual improvement.\n",
    "\n",
    "[*Data Science for Business*](https://books.google.com/books?id=4ZctAAAAQBAJ&pg=PT276), Chapter 7.3: Evaluation, Baseline Performance, and Implications for Investments in Data\n",
    "\n",
    "> *Consider carefully what would be a reasonable baseline against which to compare model performance.* This is important for the data science team in order to understand whether they indeed are improving performance, and is equally important for demonstrating to stakeholders that mining the data has added value.\n",
    "\n",
    "### What does baseline mean?\n",
    "\n",
    "Baseline is an overloaded term, as you can see in the links above. Baseline has multiple meanings:\n",
    "\n",
    "#### The score you'd get by guessing\n",
    "\n",
    "> A baseline for classification can be the most common class in the training dataset.\n",
    "\n",
    "> A baseline for regression can be the mean of the training labels. \n",
    "\n",
    "> A baseline for time-series regressions can be the value from the previous timestep. ‚Äî[Will Koehrsen](https://twitter.com/koehrsen_will/status/1088863527778111488)\n",
    "\n",
    "#### Fast, first models that beat guessing\n",
    "\n",
    "What my mentor was talking about.\n",
    "\n",
    "#### Complete, tuned \"simpler\" model\n",
    "\n",
    "Can be simpler mathematically and computationally. For example, Logistic Regression versus Deep Learning.\n",
    "\n",
    "Or can be simpler for the data scientist, with less work. For example, a model with less feature engineering versus a model with more feature engineering.\n",
    "\n",
    "#### Minimum performance that \"matters\"\n",
    "\n",
    "To go to production and get business value.\n",
    "\n",
    "#### Human-level performance \n",
    "\n",
    "Your goal may to be match, or nearly match, human performance, but with better speed, cost, or consistency.\n",
    "\n",
    "Or your goal may to be exceed human performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7xuMi_ZNQUML",
    "outputId": "2604614f-6160-49fd-9871-7115cc3162e5"
   },
   "outputs": [],
   "source": [
    "#USE MEAN AS A BASELINE PREDICTION\n",
    "train['price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "Aft4SoKNrJP5",
    "outputId": "417ecef3-f3be-4b40-da20-8d53a11d2fd1"
   },
   "outputs": [],
   "source": [
    "first10 = test[['price']].head(10)\n",
    "first10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "buBK-rBisKO5",
    "outputId": "62d29165-8374-4cd5-bd6d-af8cfc6251ce"
   },
   "outputs": [],
   "source": [
    "#This is a series\n",
    "test['price'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "8sOmQsQTrS7N",
    "outputId": "f4a3cc06-7578-4a71-9eb6-03e6a4d0a41e"
   },
   "outputs": [],
   "source": [
    "#Notice storage of data is different if you use 2 brackets -> It's a column instead of series\n",
    "test[['price']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUDOoVmCrM1-"
   },
   "outputs": [],
   "source": [
    "#Add a column\n",
    "first10['predicted'] = [3432] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "OyrS_9J_rXWP",
    "outputId": "7eec6cd7-e2a0-47c2-9972-9525474423cc"
   },
   "outputs": [],
   "source": [
    "first10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "FOUp3nFlra95",
    "outputId": "6f8a80aa-d7ac-4c04-8ee1-0474a2666f75"
   },
   "outputs": [],
   "source": [
    "first10['error'] = first10['price'] - first10['predicted']\n",
    "first10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CBO3upk4skwG",
    "outputId": "98306fec-926d-4bf9-8baa-7015ae03f83c"
   },
   "outputs": [],
   "source": [
    "first10['error'].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txvsNu8b3H8v"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HwUSXfHe3er2",
    "outputId": "18dae7a4-f4b4-4159-f286-c220ecac5f74"
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(first10['price'],first10['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Je1yVmy3jou"
   },
   "outputs": [],
   "source": [
    "y_test = test['price']\n",
    "y_pred = [train['price'].mean()]*len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "usjq5Aws3p4e",
    "outputId": "e8dac933-bebe-4553-e25a-c4b235091ab2"
   },
   "outputs": [],
   "source": [
    "print(len(y_pred))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nNAUJKpt3sRg",
    "outputId": "e6fcd868-7539-4218-8704-3d33f65d0fde"
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8poDRMsa3uwH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "intro_predictive_modeling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
